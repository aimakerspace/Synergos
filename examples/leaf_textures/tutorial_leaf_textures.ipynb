{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a Federated Cycle with Synergos\n",
    "This notebook shows how to use Synergos to run on your local machine a simulated federated cycle through three phases: Connect, Train, Evaluate, for the [one hundred plant species leaf dataset](https://archive.ics.uci.edu/ml/datasets/One-hundred+plant+species+leaves+data+set).\n",
    "\n",
    "### About the dataset:\n",
    "* 1600 instances of leaves across 16 classes\n",
    "* Features: shape, texture and margin, each represented as a 64-element vector\n",
    "* This demo will use texture data from the data_Tex_64.txt file to perform a classification task\n",
    "\n",
    "### Prerequisites:\n",
    "Before running this notebook, you should have:\n",
    "* Built the required docker images for TTP and Worker\n",
    "* Installed Synergos in your (virtual) environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.model_selection\n",
    "\n",
    "# transform original data from data_Tex_64.txt, into format:\n",
    "#     /TARGET_DATASET_DIR # contains the datasets in this demo ready to be distributed/mounted to the worker containers\n",
    "#         /data[n] # for each worker participating\n",
    "#             /train\n",
    "#                 xxx.csv\n",
    "#                 metadata.json\n",
    "#                 schema.json\n",
    "#             /evaluate\n",
    "#                 yyy.csv\n",
    "#                 metadata.json\n",
    "#                 schema.json\n",
    "#             /predict\n",
    "#                 zzz.csv\n",
    "#                 metadata.json\n",
    "#                 schema.json\n",
    "\n",
    "ORIGINAL_DATA_FILE = \"dataset/data_Tex_64.txt\"   # path to the downloaded texture data file \n",
    "TARGET_DATASET_DIR = \"leaf_textures\"\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "os.makedirs(TARGET_DATASET_DIR, exist_ok=True)\n",
    "    \n",
    "# Transform the csv according to the required format\n",
    "# Shape and margin can be included by merging into one dataframe, if \n",
    "def transform(filepath):\n",
    "    df = pd.read_csv(filepath, header=None)\n",
    "    df.rename(columns={0:'target'}, inplace=True) # rename the target column\n",
    "    return df\n",
    "\n",
    "df = transform(ORIGINAL_DATA_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string labels to numeric as only numeric labels are supported\n",
    "mapping = {}\n",
    "n = 0\n",
    "for x in df['target'].unique():\n",
    "    mapping[x] = str(n)\n",
    "    n = n+1\n",
    "df.replace(to_replace=mapping, inplace=True)\n",
    "\n",
    "# save the mapping for future reference\n",
    "with open('target_mapping.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(mapping, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NAs\n",
    "print(\"NAs:\", df[df.isna().any(axis=1)])\n",
    "\n",
    "# Convert int to float\n",
    "dfs = df.select_dtypes(include=\"int64\")\n",
    "df = df.astype({c: 'float64' for c in dfs.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and create train/evaluate/predict splits\n",
    "\n",
    "train_proportion = 0.6\n",
    "evaluate_proportion = 0.2\n",
    "\n",
    "df = sklearn.utils.shuffle(df)\n",
    "\n",
    "train_data, evaluate_data, predict_data = np.split(df, [int(train_proportion*len(df)), int((train_proportion+evaluate_proportion)*len(df))])\n",
    "\n",
    "data_dict = {\n",
    "    \"train\": train_data,\n",
    "    \"evaluate\": evaluate_data,\n",
    "    \"predict\": predict_data\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create folder structure and save data and metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create schema.json and metadata.json files\n",
    "schema = {}\n",
    "for col_name in df.columns:\n",
    "    if col_name == \"target\":\n",
    "        schema[col_name] = 'category'\n",
    "    else:\n",
    "        schema[str(col_name)] = str(df[col_name].dtype)\n",
    "\n",
    "metadata = {\n",
    "    \"datatype\": \"tabular\",\n",
    "    \"operations\": {}\n",
    "}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For train, evaluate and predict data, create worker splits \n",
    "# and write to each worker's directories\n",
    "worker_splits = np.array_split(train_data, NUM_WORKERS)\n",
    "\n",
    "for i in range(NUM_WORKERS):\n",
    "\n",
    "    worker_data_dir = TARGET_DATASET_DIR + \"/data\" + str(i+1)\n",
    "    for subcategory in [\"train\", \"evaluate\", \"predict\"]:\n",
    "        worker_subdir = worker_data_dir + \"/\" + subcategory\n",
    "        os.makedirs(worker_subdir, exist_ok=True)\n",
    "        \n",
    "        # Retrieve data from data_dict and get split for this worker\n",
    "        current_data = np.array_split(data_dict[subcategory], NUM_WORKERS)[i]\n",
    "        current_data.to_csv(worker_subdir + '/' + subcategory + '.csv', index=False) # For example: data1/predict/predict.csv\n",
    "            \n",
    "        with open(worker_subdir + '/schema.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(schema, f, indent=4)\n",
    "        with open(worker_subdir + '/metadata.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set up TTP and worker nodes\n",
    "\n",
    "#### Running on local machine with local participants:\n",
    "\n",
    "Execute these commands from within your working project directory, in separate terminals\n",
    "\n",
    "````\n",
    "docker run -v \"$(pwd)\"/leaf_textures/data1:/worker/data -v \"$(pwd)\"/demo_outputs/worker1:/worker/outputs --name worker_1 worker:pysyft_demo\n",
    "\n",
    "docker run -v \"$(pwd)\"/leaf_textures/data2:/worker/data -v \"$(pwd)\"/demo_outputs/worker2:/worker/outputs --name worker_2 worker:pysyft_demo\n",
    "\n",
    "docker run -p 5000:5000 -p 5678:5678 -p 8020:8020 -p 8080:8080 -v \"$(pwd)\"/demo_outputs:/ttp/mlflow -v \"$(pwd)\"/leaf_textures/ttp_data:/ttp/data -v \"$(pwd)\"/demo_outputs/ttp:/ttp/outputs --name ttp --link worker_1 --link worker_2 ttp:pysyft_demo\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Start project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synergos import Driver\n",
    "\n",
    "host = \"0.0.0.0\"    # IP and port of TTP service\n",
    "port = 5000\n",
    "\n",
    "driver = Driver(host=host, port=port)\n",
    "\n",
    "############################################################\n",
    "# Phase 1: CONNECT - Submitting TTP & Participant metadata #\n",
    "############################################################\n",
    "\n",
    "# 1A. TTP controller creates a project\n",
    "\n",
    "driver.projects.create(\n",
    "    project_id=\"test_project\",\n",
    "    action=\"classify\",     # either regress or classify\n",
    "    incentives={\n",
    "        'tier_1': [],\n",
    "        'tier_2': [],\n",
    "        'tier_3': []\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# 1B. TTP controller creates an experiment\n",
    "\n",
    "# Define a simple two layer neural net\n",
    "driver.experiments.create(\n",
    "    project_id=\"test_project\",\n",
    "    expt_id=\"test_experiment\",\n",
    "    model=[\n",
    "        {\n",
    "            \"activation\": \"sigmoid\",\n",
    "            \"is_input\": True,\n",
    "            \"l_type\": \"Linear\",\n",
    "            \"structure\": {\n",
    "                \"bias\": True,\n",
    "                \"in_features\": 64,\n",
    "                \"out_features\": 32\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"activation\": \"softmax\",\n",
    "            \"is_input\": False,\n",
    "            \"l_type\": \"Linear\",\n",
    "            \"structure\": {\n",
    "                \"bias\": True,\n",
    "                \"in_features\": 32,\n",
    "                \"out_features\": 16\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# 1C. TTP controller creates a run\n",
    "\n",
    "driver.runs.create(\n",
    "    project_id=\"test_project\",\n",
    "    expt_id=\"test_experiment\",\n",
    "    run_id=\"test_run\",\n",
    "    rounds=2,\n",
    "    epochs=1,\n",
    "    base_lr=0.0005,\n",
    "    max_lr=0.005,\n",
    "    criterion=\"NLLLoss\"\n",
    ")\n",
    "\n",
    "\n",
    "# 1D. Participants registers their servers' configurations\n",
    "\n",
    "driver.participants.create(\n",
    "    participant_id=\"test_participant_1\",\n",
    "    host='172.17.0.2',       # IP of the worker container\n",
    "    port=8020,\n",
    "    f_port=5000,\n",
    "    log_msgs=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "driver.participants.create(\n",
    "    participant_id=\"test_participant_2\",\n",
    "    host='172.17.0.3',\n",
    "    port=8020,\n",
    "    f_port=5000,\n",
    "    log_msgs=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "# 1E. Participants registers their role in a specific project\n",
    "# Roles: Host: contribute data # Guest: has validation set, may also contribute data.\n",
    "\n",
    "driver.registrations.create(\n",
    "    project_id=\"test_project\",\n",
    "    participant_id=\"test_participant_1\",\n",
    "    role=\"guest\"              \n",
    ")\n",
    "\n",
    "driver.registrations.create(\n",
    "    project_id=\"test_project\",\n",
    "    participant_id=\"test_participant_2\",\n",
    "    role=\"host\"\n",
    ")\n",
    "\n",
    "\n",
    "# 1F. Participants registers their tags for a specific project\n",
    "\n",
    "driver.tags.create(\n",
    "    project_id=\"test_project\",\n",
    "    participant_id=\"test_participant_1\",\n",
    "    train=[[\"train\"]],\n",
    "    evaluate=[[\"evaluate\"]],\n",
    ")\n",
    "\n",
    "driver.tags.create(\n",
    "    project_id=\"test_project\",\n",
    "    participant_id=\"test_participant_2\",\n",
    "    train=[[\"train\"]],\n",
    "    evaluate=[[\"evaluate\"]]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Phase 2: TRAIN - Alignment, Training & Optimisation #\n",
    "#######################################################\n",
    "\n",
    "# 2A. Perform multiple feature alignment to dynamically configure datasets and models for cross-grid compatibility\n",
    "\n",
    "driver.alignments.create(project_id=\"test_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2B. Trigger training across the federated grid\n",
    "\n",
    "model_resp = driver.models.create(\n",
    "    project_id=\"test_project\",\n",
    "    expt_id=\"test_experiment\",\n",
    "    run_id=\"test_run\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Phase 3: EVALUATE - Validation & Predictions #\n",
    "################################################\n",
    "\n",
    "# 3A. Perform validation(s) of combination(s)\n",
    "\n",
    "driver.validations.create(\n",
    "    project_id=\"test_project\",\n",
    "    expt_id=\"test_experiment\",\n",
    "    run_id=\"test_run\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3B. Perform prediction(s) of combination(s)\n",
    "\n",
    "driver.predictions.create(\n",
    "    tags={\"test_project\": [[\"predict\"]]},\n",
    "    participant_id=\"test_participant_1\",\n",
    "    project_id=\"test_project\",\n",
    "    expt_id=\"test_experiment\",\n",
    "    run_id=\"test_run\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3B. Perform prediction(s) of combination(s)\n",
    "\n",
    "driver.predictions.create(\n",
    "    tags={\"test_project\": [[\"predict\"]]},\n",
    "    participant_id=\"test_participant_2\",\n",
    "    project_id=\"test_project\",\n",
    "    expt_id=\"test_experiment\",\n",
    "    run_id=\"test_run\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
